<!-- livebook:{"persist_outputs":true} -->

# Elixir Micrograd

```elixir
Mix.install([
  {:kino, "~> 0.15.3"},
  {:tucan, "~> 0.4.1"},
  {:kino_vega_lite, "~> 0.1.13"}
])
```

## Basic concepts

After attempting to understand how neural networks actually operate from first principles for some time, I stumbled up Andrej Karpathy's micrograd library and the video he posted walking through how to build it from scratch.

[micrograd github](https://github.com/karpathy/micrograd)

[The spelled-out intro to neural networks and backpropagation: building micrograd](https://youtu.be/VMj-3S1tku0)

This livebook recreates micrograd in Elixir, using mermaid diagrams to visualize the computational graphs.

### Basic math knowledge required

In order to train neural networks we need to understand the impact of various parts of the calculation so that we can then adjust them accordingly.

Say we have the equation

$x * y + z$

### Calculus Chain Rule

$\frac{dz}{dx} = \frac{dz}{dy} . \frac{dy}{dx}$

> Intuitively, the chain rule states that knowing the instantaneous rate of change of z relative to y and that of y relative to x allows one to calculate the instantaneous rate of change of z relative to x as the product of the two rates of change.
> 
> If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 Ã— 4 = 8 times as fast as the man.

[Wikipedia Chain Rule](https://en.wikipedia.org/wiki/Chain_rule)

If we expand this out to elixir code we would get:

```elixir
a = 2.0
b = 3.0

c = a * b

d = 4

e = c + d
```

<!-- livebook:{"output":true} -->

```
10.0
```

If we then want to calculate the gradient for each value with respect to $e$:

$$
\frac{da}{de} = \frac{da}{dc} . \frac{dc}{de}
$$

## Value struct

```elixir
defmodule Value do
  defstruct [:data, :grad, :op, :label, :id, children: []]

  @type t :: %__MODULE__{
    data: float(),
    grad: float(),
    children: list(__MODULE__.t()),
    op: String.t(),
    label: String.t(),
    id: integer(),
  }

  def new(data, label),
    do: %__MODULE__{data: data, label: label} |> gen_id()
  
  def gen_id(%__MODULE__{} = v), do: %__MODULE__{v | id: Enum.random(0..1000)}
end
```

<!-- livebook:{"output":true} -->

```
{:module, Value, <<70, 79, 82, 49, 0, 0, 16, ...>>, {:gen_id, 1}}
```

```elixir
a = Value.new(1.0, "a")
b = Value.new(2.0, "b")
```

<!-- livebook:{"output":true} -->

```
%Value{data: 2.0, grad: nil, op: nil, label: "b", id: 317, children: []}
```

```elixir
defmodule Math do
  def add(value_one, value_two, result_label) do
    %Value{
      data: value_one.data + value_two.data,
      children: [value_one, value_two],
      label: result_label,
      op: "add"
    }
    |> Value.gen_id()
  end

  def sum(values, result_label) do
    %Value{
      data: Enum.reduce(values, 0.0, & &1.data + &2),
      children: values,
      label: result_label,
      op: "sum"
    }
    |> Value.gen_id()
  end

  def mul(value_one, value_two, result_label) do
    %Value{
      data: value_one.data * value_two.data,
      children: [value_one, value_two],
      label: result_label,
      op: "mul"
    }
    |> Value.gen_id()
  end

  def pow(value, exp, result_label) do
    %Value{
      data: value.data ** exp.data,
      children: [value, exp],
      label: result_label,
      op: "pow"
    }
    |> Value.gen_id()
  end

  def tanh(value, result_label) do
    if (value.data > 1000.0) do
      IO.inspect(value)
    end
    
    x = value.data
    t = (:math.exp(2 * x) - 1) / (:math.exp(2 * x) + 1)
    
    %Value{
      data: t,
      children: [value],
      label: result_label,
      op: "tanh"
    }
    |> Value.gen_id()
  end

  def relu(value, result_label) do
    %Value{
      data: Enum.max([value.data, 0]),
      children: [value],
      label: result_label,
      op: "relu"
    }
    |> Value.gen_id()
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Math, <<70, 79, 82, 49, 0, 0, 20, ...>>, {:relu, 2}}
```

```elixir
defmodule Mermaid do
  def render_value(value) do
    """
    flowchart LR
    #{to_mermaid(value)}
    """
    |> Kino.Mermaid.new()
  end

  def raw_txt(value) do
    """
    flowchart LR
    #{to_mermaid(value)}
    """
  end

  defp to_mermaid(nil), do: ""

  defp to_mermaid(%{children: []} = value) do
    value_node(value)
  end

  defp to_mermaid(%{children: children, op: op} = value) do
    children_ids = Enum.reduce(children, "", & &2 <> to_string(&1.id))
    operator_node_id = "#{children_ids}#{op}"

    """
    #{
      for child <- children, reduce: "" do
        acc ->
         acc <> "\n #{to_mermaid(child)} --> #{operator_node_id}[#{op}]"
      end
    }
    #{operator_node_id}[#{op}] --> #{value_node(value)}
    """
  end

  defp value_node(%{id: id, data: data, label: label, grad: grad}) do
    """
    #{id}[#{label}
    data: #{data}
    grad: #{grad}]
    """
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Mermaid, <<70, 79, 82, 49, 0, 0, 17, ...>>, {:value_node, 1}}
```

```elixir
defmodule Topo do
  def construct_topo(value) do
    {topo, _visited} = build_topo(value, [], MapSet.new)
    topo
  end

  defp build_topo(nil, topo, visited), do: {topo, visited}
  
  defp build_topo(value, topo, visited) do
    if !MapSet.member?(visited, value) do
      visited = MapSet.put(visited, value)

      {topo, visited} =
        for child <- value.children, reduce: {topo, visited} do
          {topo, visited} -> build_topo(child, topo, visited)
        end
      
      {[value | topo], visited}
    else
      {topo, visited}
    end
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Topo, <<70, 79, 82, 49, 0, 0, 11, ...>>, {:build_topo, 3}}
```

```elixir
defmodule Backward do
  def compute_grads(root) do
    nodes = Topo.construct_topo(root)
    
    Enum.reduce(nodes, %{root.id => 1.0}, &gradients_for_node/2)
  end

  defp gradients_for_node(node, grad_map) do
    node_grad = Map.get(grad_map, node.id, 0.0)
    child_grads = calc_gradients(node, node_grad)

    for {child_id, grad} <- child_grads, reduce: grad_map do
      acc ->
        Map.update(
          acc,
          child_id,
          grad,
          fn existing_grad -> existing_grad + grad end
        )
    end
  end

  defp calc_gradients(value, value_gradient)

  defp calc_gradients(%Value{children: []}, _), do: []
  
  defp calc_gradients(%Value{op: "add", children: [left, right]}, g),
    do: [{left.id, g}, {right.id, g}]

  defp calc_gradients(%Value{op: "mul", children: [left, right]}, g),
    do: [{left.id, right.data * g}, {right.id, left.data * g}]

  defp calc_gradients(%Value{op: "tanh", children: [child], data: d}, g),
    do: [{child.id, (1 - d ** 2) * g}]

  defp calc_gradients(%Value{op: "pow", children: [child, exp]}, g),
    do: [{child.id, (exp.data * child.data ** (exp.data - 1)) * g}]

  defp calc_gradients(%Value{op: "sum", children: children}, g) do
    for child <- children, do: {child.id, g}
  end

  defp calc_gradients(%Value{op: "relu", children: [child], data: d}, g) do
    grad =
      if d > 0 do
        g
      else
        0
      end
    
    [{child.id, grad}]
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Backward, <<70, 79, 82, 49, 0, 0, 20, ...>>, {:calc_gradients, 2}}
```

```elixir
defmodule Rebuild do
  def rebuild(%Value{} = val, grad_map) do
    %Value{val |
      children: Enum.map(val.children, &rebuild(&1, grad_map)),
      grad: Map.get(grad_map, val.id)
    }
  end
end

```

<!-- livebook:{"output":true} -->

```
{:module, Rebuild, <<70, 79, 82, 49, 0, 0, 10, ...>>, {:rebuild, 2}}
```

```elixir
exp = Value.new(2.0, "exp")

x = Math.add(a, b, "c") |> Math.pow(exp, "sq") |> Math.tanh("tanh")

grads = Backward.compute_grads(x)

Rebuild.rebuild(x, grads)
|> Mermaid.render_value()
```

<!-- livebook:{"output":true} -->

```mermaid
flowchart LR

 
 
 138[a
data: 1.0
grad: 3.655195026652791e-7]
 --> 138317add[add]
 317[b
data: 2.0
grad: 3.655195026652791e-7]
 --> 138317add[add]
138317add[add] --> 423[c
data: 3.0
grad: 3.655195026652791e-7]

 --> 423922pow[pow]
 922[exp
data: 2.0
grad: ]
 --> 423922pow[pow]
423922pow[pow] --> 191[sq
data: 9.0
grad: 6.091991711087985e-8]

 --> 191tanh[tanh]
191tanh[tanh] --> 332[tanh
data: 0.999999969540041
grad: 1.0]



```

```elixir
# Creating a single neron

# Given two input neurons
x1 = Value.new(2.0, "x1")
x2 = Value.new(0.0, "x2")

# And weights for each input
w1 = Value.new(-3.0, "w1")
w2 = Value.new(1.0, "w2")

# Bias of the neuron
# The specific value is so backpropagation uses nice values
b = Value.new(6.8813735870195432, "b")

x1w1 = Math.mul(x1, w1, "x1*w1")
x2w2 = Math.mul(x2, w2, "x2*w2")

x1w1x2w2 = Math.add(x1w1, x2w2, "x1*w1+x2*w2")

n = Math.add(x1w1x2w2, b, "n")

o = Math.tanh(n, "o")
o = %Value{o | grad: 1.0}

Mermaid.render_value(o)
```

<!-- livebook:{"output":true} -->

```mermaid
flowchart LR

 
 
 
 471[x1
data: 2.0
grad: ]
 --> 471916mul[mul]
 916[w1
data: -3.0
grad: ]
 --> 471916mul[mul]
471916mul[mul] --> 883[x1*w1
data: -6.0
grad: ]

 --> 883159add[add]
 
 406[x2
data: 0.0
grad: ]
 --> 406380mul[mul]
 380[w2
data: 1.0
grad: ]
 --> 406380mul[mul]
406380mul[mul] --> 159[x2*w2
data: 0.0
grad: ]

 --> 883159add[add]
883159add[add] --> 456[x1*w1+x2*w2
data: -6.0
grad: ]

 --> 456935add[add]
 935[b
data: 6.881373587019543
grad: ]
 --> 456935add[add]
456935add[add] --> 665[n
data: 0.8813735870195432
grad: ]

 --> 665tanh[tanh]
665tanh[tanh] --> 959[o
data: 0.7071067811865476
grad: 1.0]



```

```elixir
grads = Backward.compute_grads(o)
o = Rebuild.rebuild(o, grads)

Mermaid.render_value(o)
```

<!-- livebook:{"output":true} -->

```mermaid
flowchart LR

 
 
 
 471[x1
data: 2.0
grad: -1.4999999999999996]
 --> 471916mul[mul]
 916[w1
data: -3.0
grad: 0.9999999999999998]
 --> 471916mul[mul]
471916mul[mul] --> 883[x1*w1
data: -6.0
grad: 0.4999999999999999]

 --> 883159add[add]
 
 406[x2
data: 0.0
grad: 0.4999999999999999]
 --> 406380mul[mul]
 380[w2
data: 1.0
grad: 0.0]
 --> 406380mul[mul]
406380mul[mul] --> 159[x2*w2
data: 0.0
grad: 0.4999999999999999]

 --> 883159add[add]
883159add[add] --> 456[x1*w1+x2*w2
data: -6.0
grad: 0.4999999999999999]

 --> 456935add[add]
 935[b
data: 6.881373587019543
grad: 0.4999999999999999]
 --> 456935add[add]
456935add[add] --> 665[n
data: 0.8813735870195432
grad: 0.4999999999999999]

 --> 665tanh[tanh]
665tanh[tanh] --> 959[o
data: 0.7071067811865476
grad: 1.0]



```

```elixir
defmodule Neuron do
  defstruct [:weights, :bias]
  
  def new(number_input_neurons) do
    weights =
      for i <- 1..number_input_neurons do
        Value.new(:rand.uniform() * 2 - 1, "w#{i}")
      end
    
    %__MODULE__{
      weights: weights,
      bias: Value.new(:rand.uniform() * 2 - 1, "b")
    }
  end

  def call(%__MODULE__{weights: weights, bias: bias}, inputs) do
    inputs
    |> Enum.zip(weights)
    |> Enum.map(fn {x, w} -> Math.mul(x, w, x.label <> w.label) end)
    |> Math.sum("x.w")
    |> Math.add(bias, "n")
    |> Math.tanh("output")
  end

  def train(%__MODULE__{weights: weights, bias: b} = n, grads, step \\ 0.01) do
    updated_weights = 
      Enum.map(weights, fn w ->
        grad = Map.get(grads, w.id)

        %Value{w | data: w.data + (grad * -step)}
      end)
    
    %__MODULE__{
      n |
        weights: updated_weights,
        bias: %Value{b | data: b.data + (Map.get(grads, b.id) * -step)}
    }
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Neuron, <<70, 79, 82, 49, 0, 0, 26, ...>>, {:train, 3}}
```

```elixir
Neuron.new(2)
|> Neuron.call([x1, x2])
|> Mermaid.render_value()
```

<!-- livebook:{"output":true} -->

```mermaid
flowchart LR

 
 
 
 471[x1
data: 2.0
grad: ]
 --> 471801mul[mul]
 801[w1
data: 0.18440968930165758
grad: ]
 --> 471801mul[mul]
471801mul[mul] --> 374[x1w1
data: 0.36881937860331515
grad: ]

 --> 374945sum[sum]
 
 406[x2
data: 0.0
grad: ]
 --> 406594mul[mul]
 594[w2
data: -0.2087461171208329
grad: ]
 --> 406594mul[mul]
406594mul[mul] --> 945[x2w2
data: -0.0
grad: ]

 --> 374945sum[sum]
374945sum[sum] --> 996[x.w
data: 0.36881937860331515
grad: ]

 --> 996758add[add]
 758[b
data: 0.32494524143200043
grad: ]
 --> 996758add[add]
996758add[add] --> 705[n
data: 0.6937646200353156
grad: ]

 --> 705tanh[tanh]
705tanh[tanh] --> 347[output
data: 0.6003950148753732
grad: ]



```

```elixir
defmodule Layer do
  defstruct [:neurons]

  def new(number_inputs, number_of_neurons) do
    neurons =
      for _ <- 1..number_of_neurons do
        Neuron.new(number_inputs)
      end

    %__MODULE__{neurons: neurons}
  end

  def call(%Layer{neurons: neurons}, inputs) do
    for n <- neurons do
      Neuron.call(n, inputs)
    end
  end

  def train(layer, grads, step) do
    %__MODULE__{neurons: Enum.map(layer.neurons, & Neuron.train(&1, grads, step))}
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Layer, <<70, 79, 82, 49, 0, 0, 17, ...>>, {:train, 3}}
```

```elixir
Layer.new(2, 2)
|> Layer.call([x1, x2])
|> Math.sum("o")
|> Mermaid.render_value()
```

<!-- livebook:{"output":true} -->

```mermaid
flowchart LR

 
 
 
 
 471[x1
data: 2.0
grad: ]
 --> 471664mul[mul]
 664[w1
data: -0.36642598886388655
grad: ]
 --> 471664mul[mul]
471664mul[mul] --> 791[x1w1
data: -0.7328519777277731
grad: ]

 --> 791285sum[sum]
 
 406[x2
data: 0.0
grad: ]
 --> 40685mul[mul]
 85[w2
data: 0.24409349169277395
grad: ]
 --> 40685mul[mul]
40685mul[mul] --> 285[x2w2
data: 0.0
grad: ]

 --> 791285sum[sum]
791285sum[sum] --> 863[x.w
data: -0.7328519777277731
grad: ]

 --> 863669add[add]
 669[b
data: 0.5251697652072103
grad: ]
 --> 863669add[add]
863669add[add] --> 559[n
data: -0.20768221252056285
grad: ]

 --> 559tanh[tanh]
559tanh[tanh] --> 328[output
data: -0.20474693422201704
grad: ]

 --> 328950sum[sum]
 
 
 
 
 471[x1
data: 2.0
grad: ]
 --> 471942mul[mul]
 942[w1
data: -0.7613844056812147
grad: ]
 --> 471942mul[mul]
471942mul[mul] --> 880[x1w1
data: -1.5227688113624294
grad: ]

 --> 880834sum[sum]
 
 406[x2
data: 0.0
grad: ]
 --> 406527mul[mul]
 527[w2
data: -0.07472254500614173
grad: ]
 --> 406527mul[mul]
406527mul[mul] --> 834[x2w2
data: -0.0
grad: ]

 --> 880834sum[sum]
880834sum[sum] --> 190[x.w
data: -1.5227688113624294
grad: ]

 --> 190472add[add]
 472[b
data: -0.08754549039996062
grad: ]
 --> 190472add[add]
190472add[add] --> 507[n
data: -1.61031430176239
grad: ]

 --> 507tanh[tanh]
507tanh[tanh] --> 950[output
data: -0.9232064616420873
grad: ]

 --> 328950sum[sum]
328950sum[sum] --> 764[o
data: -1.1279533958641044
grad: ]



```

```elixir
defmodule MLP do
  defstruct [:layers]

  def new(number_of_inputs, number_of_outputs) do
    size = [number_of_inputs | number_of_outputs]
    len = Enum.count(number_of_outputs) - 1

    layers =
      for i <- 0..len do
        Layer.new(Enum.at(size, i), Enum.at(size, i + 1))
      end
    
    %__MODULE__{layers: layers}
  end

  def call(%__MODULE__{layers: layers}, inputs) do
    for layer <- layers, reduce: inputs do
      inputs ->
        Layer.call(layer, inputs)
    end
  end

  def train(mlp, grads, step) do
    %__MODULE__{layers: Enum.map(mlp.layers, & Layer.train(&1, grads, step))}
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, MLP, <<70, 79, 82, 49, 0, 0, 18, ...>>, {:train, 3}}
```

```elixir
x1 = Value.new(2.0, "x1")
x2 = Value.new(3.0, "x2")
x3 = Value.new(-1.0, "x3")

mlp = MLP.new(3, [4, 4, 1])

mlp
|> MLP.call([x1, x2, x3])
# |> hd()
# |> Mermaid.render_value()
```

<!-- livebook:{"output":true} -->

```
[
  %Value{
    data: 0.6603087515779591,
    grad: nil,
    op: "tanh",
    label: "output",
    id: 11,
    children: [
      %Value{
        data: 0.793360873407211,
        grad: nil,
        op: "add",
        label: "n",
        id: 27,
        children: [
          %Value{
            data: 0.8818281906240829,
            grad: nil,
            op: "sum",
            label: "x.w",
            id: 693,
            children: [
              %Value{
                data: 0.5347037293685436,
                grad: nil,
                op: "mul",
                label: "outputw1",
                id: 526,
                children: [
                  %Value{
                    data: 0.548897643435227,
                    grad: nil,
                    op: "tanh",
                    label: "output",
                    id: 941,
                    children: [
                      %Value{
                        data: 0.6168022453335086,
                        grad: nil,
                        op: "add",
                        label: "n",
                        id: 126,
                        children: [
                          %Value{
                            data: 0.5239065100944579,
                            grad: nil,
                            op: "sum",
                            label: "x.w",
                            id: 977,
                            children: [%Value{...}, ...]
                          },
                          %Value{
                            data: 0.0928957352390507,
                            grad: nil,
                            op: nil,
                            label: "b",
                            id: 612,
                            children: []
                          }
                        ]
                      }
                    ]
                  },
                  %Value{
                    data: 0.9741410548279057,
                    grad: nil,
                    op: nil,
                    label: "w1",
                    id: 992,
                    children: []
                  }
                ]
              },
              %Value{
                data: -0.45957741220442583,
                grad: nil,
                op: "mul",
                label: "outputw2",
                id: 889,
                children: [
                  %Value{
                    data: 0.5860209734003081,
                    grad: nil,
                    op: "tanh",
                    label: "output",
                    id: 234,
                    children: [
                      %Value{
                        data: 0.6715841570192422,
                        grad: nil,
                        op: "add",
                        label: "n",
                        id: 551,
                        children: [
                          %Value{
                            data: 1.6367349099126525,
                            grad: nil,
                            op: "sum",
                            label: "x.w",
                            id: 820,
                            children: [...]
                          },
                          %Value{
                            data: -0.9651507528934102,
                            grad: nil,
                            op: nil,
                            label: "b",
                            id: 549,
                            ...
                          }
                        ]
                      }
                    ]
                  },
                  %Value{
                    data: -0.7842337272295725,
                    grad: nil,
                    op: nil,
                    label: "w2",
                    id: 472,
                    children: []
                  }
                ]
              },
              %Value{
                data: 0.3110563460594714,
                grad: nil,
                op: "mul",
                label: "outputw3",
                id: 477,
                children: [
                  %Value{
                    data: 0.6553135027481627,
                    grad: nil,
                    op: "tanh",
                    label: "output",
                    id: 487,
                    children: [
                      %Value{
                        data: 0.7845551986111658,
                        grad: nil,
                        op: "add",
                        label: "n",
                        id: 982,
                        children: [
                          %Value{
                            data: 1.044801974414714,
                            grad: nil,
                            op: "sum",
                            label: "x.w",
                            id: 822,
                            ...
                          },
                          %Value{data: -0.26024677580354805, grad: nil, op: nil, label: "b", ...}
                        ]
                      }
                    ]
                  },
                  %Value{
                    data: 0.4746679944103189,
                    grad: nil,
                    op: nil,
                    label: "w3",
                    id: 693,
                    children: []
                  }
                ]
              },
              %Value{
                data: 0.4956455274004937,
                grad: nil,
                op: "mul",
                label: "outputw4",
                id: 820,
                children: [
                  %Value{
                    data: 0.5245229375524052,
                    grad: nil,
                    op: "tanh",
                    label: "output",
                    id: 720,
                    children: [
                      %Value{
                        data: 0.5825590851252967,
                        grad: nil,
                        op: "add",
                        label: "n",
                        id: 479,
                        children: [
                          %Value{
                            data: -0.008976249112995743,
                            grad: nil,
                            op: "sum",
                            label: "x.w",
                            ...
                          },
                          %Value{data: 0.5915353342382925, grad: nil, op: nil, ...}
                        ]
                      }
                    ]
                  },
                  %Value{
                    data: 0.944945381632569,
                    grad: nil,
                    op: nil,
                    label: "w4",
                    id: 528,
                    children: []
                  }
                ]
              }
            ]
          },
          %Value{data: -0.08846731721687195, grad: nil, op: nil, label: "b", id: 369, children: []}
        ]
      }
    ]
  }
]
```

```elixir
defmodule Loss do
  def mean_sq_error(expected, predictions) do
    loss =
      for {ygt, yout} <- Enum.zip(expected, predictions) do
        Math.pow(
          Math.add(
            yout,
            Math.mul(
              ygt,
              Value.new(-1.0, "a"),
              "b"
            ),
            "c"
          ),
          Value.new(2, "exp"),
          "pow"
        )
      end 
      |> Math.sum("loss")

    %Value{loss | grad: 1.0}
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Loss, <<70, 79, 82, 49, 0, 0, 10, ...>>, {:mean_sq_error, 2}}
```

```elixir
to_list_of_values = fn row ->
  Enum.with_index(row, & Value.new(&1, "x#{&2}"))
end

inputs = [
  [2.0, 3.0, -1.0],
  [3.0, -1.0, 0.5],
  [0.5, 1.0, 1.0],
  [1.0, 1.0, -1.0]
]
|> Enum.map(&to_list_of_values.(&1))

expected_outputs = [1.0, -1.0, -1.0, 1.0] |> to_list_of_values.()

predictions = Enum.map(inputs, & MLP.call(mlp, &1) |> hd)

Enum.map(predictions, & Map.get(&1, :data))
```

<!-- livebook:{"output":true} -->

```
[0.6603087515779591, 0.6810926577051656, 0.8811100925481773, 0.6567478167209362]
```

```elixir
loss = Loss.mean_sq_error(expected_outputs, predictions)

loss.data
```

<!-- livebook:{"output":true} -->

```
6.597859909657197
```

```elixir
updated_mlp =
  for _ <- 0..50, reduce: mlp do
    acc ->
      new_predictions = Enum.map(inputs, & MLP.call(acc, &1) |> hd)
      loss = Loss.mean_sq_error(expected_outputs, new_predictions)
      
      IO.inspect(loss.data, label: "LOSS DATA")
      
      grads = Backward.compute_grads(loss)
      MLP.train(acc, grads, 0.1)
  end
```

<!-- livebook:{"output":true} -->

```
LOSS DATA: 6.597859909657197
LOSS DATA: 4.053198493965706
LOSS DATA: 1.3175841770642036
LOSS DATA: 1.5224248002206135
LOSS DATA: 1.7856693701359718
LOSS DATA: 0.09032279304936688
LOSS DATA: 0.05991429353300874
LOSS DATA: 0.007119192320229268
LOSS DATA: 0.007160626340291336
LOSS DATA: 0.010929920553448245
LOSS DATA: 0.03950577837430588
LOSS DATA: 0.7739263380116818
LOSS DATA: 2.383834308494042
LOSS DATA: 1.042472604866736
LOSS DATA: 5.044354398856507
LOSS DATA: 0.7237532084283772
LOSS DATA: 0.014050302418359233
LOSS DATA: 0.013118498467012223
LOSS DATA: 0.012540698964175114
LOSS DATA: 0.017706389588265082
LOSS DATA: 0.016882803121564628
LOSS DATA: 0.015872179759634692
LOSS DATA: 0.021418604362802292
LOSS DATA: 0.019941978578919148
LOSS DATA: 0.037168617920534
LOSS DATA: 0.03388781042481569
LOSS DATA: 0.029439945125246732
LOSS DATA: 0.025845388124556914
LOSS DATA: 0.02637762278475325
LOSS DATA: 0.02405765903900007
LOSS DATA: 0.02990161441773237
LOSS DATA: 0.026117677642196226
LOSS DATA: 0.052188110986564756
LOSS DATA: 0.04270520744098408
LOSS DATA: 0.07990578522072792
LOSS DATA: 0.04210331076312311
LOSS DATA: 0.12261838778736683
LOSS DATA: 0.08363775803705234
LOSS DATA: 0.04229547264761829
LOSS DATA: 0.07349053729297933
LOSS DATA: 0.04566633844272193
LOSS DATA: 0.029939243762798044
LOSS DATA: 0.02493875138525514
LOSS DATA: 0.028012012572169655
LOSS DATA: 0.029488780644709455
LOSS DATA: 0.018339844328868524
LOSS DATA: 0.01495976679352095
LOSS DATA: 0.013317640855268209
LOSS DATA: 0.01448795828620576
LOSS DATA: 0.012801557165466458
LOSS DATA: 0.010398700384304353
```

<!-- livebook:{"output":true} -->

```
%MLP{
  layers: [
    %Layer{
      neurons: [
        %Neuron{
          weights: [
            %Value{
              data: -1.8669570131865285,
              grad: nil,
              op: nil,
              label: "w1",
              id: 440,
              children: []
            },
            %Value{data: 0.5286895812138411, grad: nil, op: nil, label: "w2", id: 989, children: []},
            %Value{data: 2.860108542562639, grad: nil, op: nil, label: "w3", id: 416, children: []}
          ],
          bias: %Value{
            data: -1.1247313584840608,
            grad: nil,
            op: nil,
            label: "b",
            id: 30,
            children: []
          }
        },
        %Neuron{
          weights: [
            %Value{
              data: -2.2543509652171827,
              grad: nil,
              op: nil,
              label: "w1",
              id: 707,
              children: []
            },
            %Value{
              data: -1.3451065092208916,
              grad: nil,
              op: nil,
              label: "w2",
              id: 762,
              children: []
            },
            %Value{
              data: -1.4145972247943583,
              grad: nil,
              op: nil,
              label: "w3",
              id: 314,
              children: []
            }
          ],
          bias: %Value{
            data: -1.372915338487819,
            grad: nil,
            op: nil,
            label: "b",
            id: 383,
            children: []
          }
        },
        %Neuron{
          weights: [
            %Value{
              data: 0.26665557673571827,
              grad: nil,
              op: nil,
              label: "w1",
              id: 319,
              children: []
            },
            %Value{data: -0.7173840316778172, grad: nil, op: nil, label: "w2", id: 23, children: []},
            %Value{data: 2.1321936759246847, grad: nil, op: nil, label: "w3", id: 300, children: []}
          ],
          bias: %Value{
            data: 1.3106192039785245,
            grad: nil,
            op: nil,
            label: "b",
            id: 663,
            children: []
          }
        },
        %Neuron{
          weights: [
            %Value{data: 3.3704694943078297, grad: nil, op: nil, label: "w1", id: 926, children: []},
            %Value{data: 1.9564194695694543, grad: nil, op: nil, label: "w2", id: 554, children: []},
            %Value{
              data: -1.3627062887179424,
              grad: nil,
              op: nil,
              label: "w3",
              id: 625,
              children: []
            }
          ],
          bias: %Value{
            data: -0.2067919649888856,
            grad: nil,
            op: nil,
            label: "b",
            id: 968,
            children: []
          }
        }
      ]
    },
    %Layer{
      neurons: [
        %Neuron{
          weights: [
            %Value{
              data: 0.24624783789685475,
              grad: nil,
              op: nil,
              label: "w1",
              id: 628,
              children: []
            },
            %Value{
              data: 0.16231547583164904,
              grad: nil,
              op: nil,
              label: "w2",
              id: 319,
              children: []
            },
            %Value{data: -1.507501387291408, grad: nil, op: nil, label: "w3", id: 545, children: []},
            %Value{data: 0.8163131539167873, grad: nil, op: nil, label: "w4", id: 494, children: []}
          ],
          bias: %Value{
            data: -0.7733515205183088,
            grad: nil,
            op: nil,
            label: "b",
            id: 612,
            children: []
          }
        },
        %Neuron{
          weights: [
            %Value{
              data: -1.3236841265012496,
              grad: nil,
              op: nil,
              label: "w1",
              id: 627,
              children: []
            },
            %Value{
              data: -0.3084429618977618,
              grad: nil,
              op: nil,
              label: "w2",
              id: 915,
              children: []
            },
            %Value{
              data: -0.3513095713132273,
              grad: nil,
              op: nil,
              label: "w3",
              id: 490,
              children: []
            },
            %Value{
              data: 0.05243147635468927,
              grad: nil,
              op: nil,
              label: "w4",
              id: 997,
              children: []
            }
          ],
          bias: %Value{
            data: -0.49681001898504334,
            grad: nil,
            op: nil,
            label: "b",
            id: 549,
            children: []
          }
        },
        %Neuron{
          weights: [
            %Value{data: 0.8948897107599995, grad: nil, op: nil, label: "w1", id: 134, children: []},
            %Value{
              data: -0.20407834852331933,
              grad: nil,
              op: nil,
              label: "w2",
              id: 66,
              children: []
            },
            %Value{data: -2.545907789639769, grad: nil, op: nil, label: "w3", id: 868, children: []},
            %Value{
              data: -0.2821452489513278,
              grad: nil,
              op: nil,
              label: "w4",
              id: 396,
              children: []
            }
          ],
          bias: %Value{
            data: -0.3719916579204391,
            grad: nil,
            op: nil,
            label: "b",
            id: 792,
            children: []
          }
        },
        %Neuron{
          weights: [
            %Value{
              data: 0.35078739101389034,
              grad: nil,
              op: nil,
              label: "w1",
              id: 232,
              children: []
            },
            %Value{data: 0.2276781840870872, grad: nil, op: nil, label: "w2", id: 539, children: []},
            %Value{
              data: -0.2533356443597778,
              grad: nil,
              op: nil,
              label: "w3",
              id: 992,
              children: []
            },
            %Value{data: -1.255655618604205, grad: nil, op: nil, label: "w4", id: 376, children: []}
          ],
          bias: %Value{
            data: 3.4375241196439447e-4,
            grad: nil,
            op: nil,
            label: "b",
            id: 659,
            children: []
          }
        }
      ]
    },
    %Layer{
      neurons: [
        %Neuron{
          weights: [
            %Value{data: 0.5082178606639498, grad: nil, op: nil, label: "w1", id: 992, children: []},
            %Value{
              data: 0.35891926655149203,
              grad: nil,
              op: nil,
              label: "w2",
              id: 472,
              children: []
            },
            %Value{data: 1.5217720795625775, grad: nil, op: nil, label: "w3", id: 693, children: []},
            %Value{
              data: 0.0022599049970576483,
              grad: nil,
              op: nil,
              label: "w4",
              id: 528,
              children: []
            }
          ],
          bias: %Value{
            data: -0.005719439505252565,
            grad: nil,
            op: nil,
            label: "b",
            id: 369,
            children: []
          }
        }
      ]
    }
  ]
}
```

```elixir
predictions = Enum.map(inputs, & MLP.call(updated_mlp, &1) |> hd)

Enum.map(predictions, & Map.get(&1, :data))
```

<!-- livebook:{"output":true} -->

```
[0.9626020765728172, -0.9433462244439887, -0.9792008418109555, 0.9380270558744623]
```
