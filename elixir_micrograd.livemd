<!-- livebook:{"persist_outputs":true} -->

# Elixir Micrograd

```elixir
Mix.install([
  {:kino, "~> 0.15.3"}
])
```

## Basic concepts

After attempting to understand how neural networks actually operate from first principles for some time, I stumbled up Andrej Karpathy's micrograd library and the video he posted walking through how to build it from scratch.

[micrograd github](https://github.com/karpathy/micrograd)

[The spelled-out intro to neural networks and backpropagation: building micrograd](https://youtu.be/VMj-3S1tku0)

This livebook recreates micrograd in Elixir, using mermaid diagrams to visualize the computational graphs.

### Basic math knowledge required

In order to train neural networks we need to understand the impact of various parts of the calculation so that we can then adjust them accordingly.

Say we have the equation

$x * y + z$

### Calculus Chain Rule

$\frac{dz}{dx} = \frac{dz}{dy} . \frac{dy}{dx}$

> Intuitively, the chain rule states that knowing the instantaneous rate of change of z relative to y and that of y relative to x allows one to calculate the instantaneous rate of change of z relative to x as the product of the two rates of change.
> 
> If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, then the car travels 2 Ã— 4 = 8 times as fast as the man.

[Wikipedia Chain Rule](https://en.wikipedia.org/wiki/Chain_rule)

If we expand this out to elixir code we would get:

```elixir
a = 2.0
b = 3.0

c = a * b

d = 4

e = c + d
```

<!-- livebook:{"output":true} -->

```
10.0
```

If we then want to calculate the gradient for each value with respect to $e$:

$$
\frac{da}{de} = \frac{da}{dc} . \frac{dc}{de}
$$

## Value struct

```elixir
defmodule Value do
  defstruct [:data, :grad, :op, :label, :id, children: []]

  @type t :: %__MODULE__{
    data: float(),
    grad: float(),
    children: list(__MODULE__.t()),
    op: String.t(),
    label: String.t(),
    id: integer(),
  }

  def new(data, label), do: %__MODULE__{data: data, label: label} |> gen_id()
  
  def gen_id(%__MODULE__{} = v), do: %__MODULE__{v | id: Enum.random(0..1000)}
end
```

<!-- livebook:{"output":true} -->

```
{:module, Value, <<70, 79, 82, 49, 0, 0, 16, ...>>, {:gen_id, 1}}
```

```elixir
a = Value.new(1.0, "a")
b = Value.new(2.0, "b")
```

<!-- livebook:{"output":true} -->

```
%Value{data: 2.0, grad: nil, op: nil, label: "b", id: 790, children: []}
```

```elixir
defmodule Math do
  def add(value_one, value_two, result_label) do
    %Value{
      data: value_one.data + value_two.data,
      children: [value_one, value_two],
      label: result_label,
      op: "add"
    }
    |> Value.gen_id()
  end

  def sum(values, result_label) do
    %Value{
      data: Enum.reduce(values, 0.0, & &1.data + &2),
      children: values,
      label: result_label,
      op: "sum"
    }
    |> Value.gen_id()
  end

  def mul(value_one, value_two, result_label) do
    %Value{
      data: value_one.data * value_two.data,
      children: [value_one, value_two],
      label: result_label,
      op: "mul"
    }
    |> Value.gen_id()
  end

  def tanh(value, result_label) do
    x = value.data
    t = (:math.exp(2 * x) - 1) / (:math.exp(2 * x) + 1)
    
    %Value{
      data: t,
      children: [value],
      label: result_label,
      op: "tanh"
    }
    |> Value.gen_id()
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Math, <<70, 79, 82, 49, 0, 0, 15, ...>>, {:tanh, 2}}
```

```elixir
defmodule Mermaid do
  def render_value(value) do
    """
    flowchart LR
    #{to_mermaid(value)}
    """
    |> Kino.Mermaid.new()
  end

  defp to_mermaid(nil), do: ""

  defp to_mermaid(%{children: []} = value) do
    value_node(value)
  end

  defp to_mermaid(%{children: [child], op: op} = value) do
    operator_node_id = "#{child.id}#{op}"

    """
    #{to_mermaid(child)} --> #{operator_node_id}[#{op}]
    #{operator_node_id}[#{op}] --> #{value_node(value)}
    """
  end

  defp to_mermaid(%{children: [left, right], op: op} = value) do
    operator_node_id = "#{left.id}#{op}#{right.id}"

    """
    #{to_mermaid(left)} --> #{operator_node_id}[#{op}]
    #{to_mermaid(right)} --> #{operator_node_id}[#{op}]
    #{operator_node_id}[#{op}] --> #{value_node(value)}
    """
  end

  defp value_node(%{id: id, data: data, label: label, grad: grad}) do
    """
    #{id}[#{label}
    data: #{data}
    grad: #{grad}]
    """
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Mermaid, <<70, 79, 82, 49, 0, 0, 17, ...>>, {:value_node, 1}}
```

```elixir
Math.add(a, b, "c")
|> Mermaid.render_value()
```

<!-- livebook:{"output":true} -->

```mermaid
flowchart LR
642[a
data: 1.0
grad: ]
 --> 642add790[add]
790[b
data: 2.0
grad: ]
 --> 642add790[add]
642add790[add] --> 303[c
data: 3.0
grad: ]



```

```elixir
defmodule Topo do
  def construct_topo(value) do
    {topo, _visited} = build_topo(value, [], MapSet.new)
    topo
  end

  defp build_topo(nil, topo, visited), do: {topo, visited}
  
  defp build_topo(value, topo, visited) do
    if !MapSet.member?(visited, value) do
      visited = MapSet.put(visited, value)

      {topo, visited} =
        for child <- value.children, reduce: {topo, visited} do
          {topo, visited} -> build_topo(child, topo, visited)
        end
      
      {[value | topo], visited}
    else
      {topo, visited}
    end
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Topo, <<70, 79, 82, 49, 0, 0, 11, ...>>, {:build_topo, 3}}
```

```elixir
defmodule Backward do
  def compute_grads(root) do
    nodes = Topo.construct_topo(root)
    
    Enum.reduce(nodes, %{root.id => 1.0}, &gradients_for_node/2)
  end

  defp gradients_for_node(node, grad_map) do
    node_grad = Map.get(grad_map, node.id, 0.0)
    child_grads = calc_gradients(node, node_grad)

    for {child_id, grad} <- child_grads, reduce: grad_map do
      acc ->
        Map.update(
          acc,
          child_id,
          grad,
          fn existing_grad -> existing_grad + grad end
        )
    end
  end

  defp calc_gradients(value, value_gradient)

  defp calc_gradients(%Value{children: []}, _), do: []
  
  defp calc_gradients(%Value{op: "add", children: [left, right]}, g),
    do: [{left.id, g}, {right.id, g}]

  defp calc_gradients(%Value{op: "mul", children: [left, right]}, g),
    do: [{left.id, right.data * g}, {right.id, left.data * g}]

  defp calc_gradients(%Value{op: "tanh", children: [child], data: d}, g),
    do: [{child.id, (1 - d ** 2) * g}]

  defp calc_gradients(%Value{op: "sum", children: children}, g) do
    for child <- children, do: {child.id, g}
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Backward, <<70, 79, 82, 49, 0, 0, 17, ...>>, {:calc_gradients, 2}}
```

```elixir
defmodule Rebuild do
  def rebuild(%Value{} = val, grad_map) do
    %Value{val |
      children: Enum.map(val.children, &rebuild(&1, grad_map)),
      grad: Map.get(grad_map, val.id)
    }
  end
end

```

<!-- livebook:{"output":true} -->

```
{:module, Rebuild, <<70, 79, 82, 49, 0, 0, 10, ...>>, {:rebuild, 2}}
```

```elixir
# Creating a single neron

# Given two input neurons
x1 = Value.new(2.0, "x1")
x2 = Value.new(0.0, "x2")

# And weights for each input
w1 = Value.new(-3.0, "w1")
w2 = Value.new(1.0, "w2")

# Bias of the neuron
# The specific value is so backpropagation uses nice values
b = Value.new(6.8813735870195432, "b")

x1w1 = Math.mul(x1, w1, "x1*w1")
x2w2 = Math.mul(x2, w2, "x2*w2")

x1w1x2w2 = Math.add(x1w1, x2w2, "x1*w1+x2*w2")

n = Math.add(x1w1x2w2, b, "n")

o = Math.tanh(n, "o")
o = %Value{o | grad: 1.0}

Mermaid.render_value(o)
```

<!-- livebook:{"output":true} -->

```mermaid
flowchart LR
290[x1
data: 2.0
grad: ]
 --> 290mul220[mul]
220[w1
data: -3.0
grad: ]
 --> 290mul220[mul]
290mul220[mul] --> 644[x1*w1
data: -6.0
grad: ]

 --> 644add453[add]
524[x2
data: 0.0
grad: ]
 --> 524mul649[mul]
649[w2
data: 1.0
grad: ]
 --> 524mul649[mul]
524mul649[mul] --> 453[x2*w2
data: 0.0
grad: ]

 --> 644add453[add]
644add453[add] --> 611[x1*w1+x2*w2
data: -6.0
grad: ]

 --> 611add646[add]
646[b
data: 6.881373587019543
grad: ]
 --> 611add646[add]
611add646[add] --> 967[n
data: 0.8813735870195432
grad: ]

 --> 967tanh[tanh]
967tanh[tanh] --> 934[o
data: 0.7071067811865476
grad: 1.0]



```

```elixir
grads = Backward.compute_grads(o)
o = Rebuild.rebuild(o, grads)

Mermaid.render_value(o)
```

<!-- livebook:{"output":true} -->

```mermaid
flowchart LR
290[x1
data: 2.0
grad: -1.4999999999999996]
 --> 290mul220[mul]
220[w1
data: -3.0
grad: 0.9999999999999998]
 --> 290mul220[mul]
290mul220[mul] --> 644[x1*w1
data: -6.0
grad: 0.4999999999999999]

 --> 644add453[add]
524[x2
data: 0.0
grad: 0.4999999999999999]
 --> 524mul649[mul]
649[w2
data: 1.0
grad: 0.0]
 --> 524mul649[mul]
524mul649[mul] --> 453[x2*w2
data: 0.0
grad: 0.4999999999999999]

 --> 644add453[add]
644add453[add] --> 611[x1*w1+x2*w2
data: -6.0
grad: 0.4999999999999999]

 --> 611add646[add]
646[b
data: 6.881373587019543
grad: 0.4999999999999999]
 --> 611add646[add]
611add646[add] --> 967[n
data: 0.8813735870195432
grad: 0.4999999999999999]

 --> 967tanh[tanh]
967tanh[tanh] --> 934[o
data: 0.7071067811865476
grad: 1.0]



```

```elixir
x = [1, 2]
y = [3, 4]

Enum.zip([x, y])
```

<!-- livebook:{"output":true} -->

```
[{1, 3}, {2, 4}]
```

```elixir
defmodule Neuron do
  defstruct [:weights, :bias]
  
  def new(number_input_neurons) do
    weights =
      for i <- 1..number_input_neurons do
        Value.new(:rand.uniform() * 2 - 1, "w#{i}")
      end
    
    %__MODULE__{
      weights: weights,
      bias: Value.new(:rand.uniform() * 2 - 1, "b")
    }
  end

  def call(%Neuron{weights: weights, bias: bias}, inputs) do
    inputs
    |> Enum.zip(weights)
    |> Enum.map(fn {x, w} -> Math.mul(x, w, x.label <> w.label) end)
    |> Math.sum("x.w")
    |> Math.add(bias, "n")
    |> Math.tanh("output")
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Neuron, <<70, 79, 82, 49, 0, 0, 18, ...>>, {:call, 2}}
```

```elixir
Neuron.new(2)
|> Neuron.call([x1, x2])
|> Mermaid.render_value()
```

<!-- livebook:{"output":true} -->

```mermaid
flowchart LR
290[x1
data: 2.0
grad: ]
 --> 290mul4[mul]
4[w1
data: 0.7639636858580996
grad: ]
 --> 290mul4[mul]
290mul4[mul] --> 944[x1w1
data: 1.5279273717161992
grad: ]

 --> 944sum459[sum]
524[x2
data: 0.0
grad: ]
 --> 524mul42[mul]
42[w2
data: -0.7783282850153421
grad: ]
 --> 524mul42[mul]
524mul42[mul] --> 459[x2w2
data: -0.0
grad: ]

 --> 944sum459[sum]
944sum459[sum] --> 540[x.w
data: 1.5279273717161992
grad: ]

 --> 540add661[add]
661[b
data: -0.3083523139529736
grad: ]
 --> 540add661[add]
540add661[add] --> 450[n
data: 1.2195750577632256
grad: ]

 --> 450tanh[tanh]
450tanh[tanh] --> 842[output
data: 0.8395287810918671
grad: ]



```

```elixir
defmodule Layer do
  defstruct [:neurons]

  def new(number_inputs, number_of_neurons) do
    neurons =
      for _ <- 1..number_of_neurons do
        Neuron.new(number_inputs)
      end

    %__MODULE__{neurons: neurons}
  end

  def call(%Layer{neurons: neurons}, inputs) do
    for n <- neurons do
      Neuron.call(n, inputs)
    end
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, Layer, <<70, 79, 82, 49, 0, 0, 14, ...>>, {:call, 2}}
```

```elixir
[n1, n2] =
  Layer.new(2, 2)
  |> Layer.call([x1, x2])
# |> Enum.reduce(
#   Value.new(0.0, "xw_acc"),
#   fn val, acc -> Math.add(val, acc, "#{val.label}#{acc.label}") end
# )
# |> Mermaid.render_value()
```

<!-- livebook:{"output":true} -->

```
[
  %Value{
    data: -0.9836681091430985,
    grad: nil,
    op: "tanh",
    label: "output",
    id: 815,
    children: [
      %Value{
        data: -2.399791649747179,
        grad: nil,
        op: "add",
        label: "n",
        id: 676,
        children: [
          %Value{
            data: -1.7071070353014148,
            grad: nil,
            op: "sum",
            label: "x.w",
            id: 183,
            children: [
              %Value{
                data: -1.7071070353014148,
                grad: nil,
                op: "mul",
                label: "x1w1",
                id: 445,
                children: [
                  %Value{data: 2.0, grad: nil, op: nil, label: "x1", id: 290, children: []},
                  %Value{
                    data: -0.8535535176507074,
                    grad: nil,
                    op: nil,
                    label: "w1",
                    id: 828,
                    children: []
                  }
                ]
              },
              %Value{
                data: -0.0,
                grad: nil,
                op: "mul",
                label: "x2w2",
                id: 72,
                children: [
                  %Value{data: 0.0, grad: nil, op: nil, label: "x2", id: 524, children: []},
                  %Value{
                    data: -0.3809673052307294,
                    grad: nil,
                    op: nil,
                    label: "w2",
                    id: 608,
                    children: []
                  }
                ]
              }
            ]
          },
          %Value{data: -0.6926846144457646, grad: nil, op: nil, label: "b", id: 391, children: []}
        ]
      }
    ]
  },
  %Value{
    data: 0.5120540432675122,
    grad: nil,
    op: "tanh",
    label: "output",
    id: 966,
    children: [
      %Value{
        data: 0.5655098218413082,
        grad: nil,
        op: "add",
        label: "n",
        id: 533,
        children: [
          %Value{
            data: 0.8633012260696216,
            grad: nil,
            op: "sum",
            label: "x.w",
            id: 535,
            children: [
              %Value{
                data: 0.8633012260696216,
                grad: nil,
                op: "mul",
                label: "x1w1",
                id: 863,
                children: [
                  %Value{data: 2.0, grad: nil, op: nil, label: "x1", id: 290, children: []},
                  %Value{
                    data: 0.4316506130348108,
                    grad: nil,
                    op: nil,
                    label: "w1",
                    id: 469,
                    children: []
                  }
                ]
              },
              %Value{
                data: 0.0,
                grad: nil,
                op: "mul",
                label: "x2w2",
                id: 437,
                children: [
                  %Value{data: 0.0, grad: nil, op: nil, label: "x2", id: 524, children: []},
                  %Value{
                    data: 0.9677210340324143,
                    grad: nil,
                    op: nil,
                    label: "w2",
                    id: 650,
                    children: []
                  }
                ]
              }
            ]
          },
          %Value{data: -0.2977914042283134, grad: nil, op: nil, label: "b", id: 47, children: []}
        ]
      }
    ]
  }
]
```

```elixir
Mermaid.render_value(n1)
```

<!-- livebook:{"output":true} -->

```mermaid
flowchart LR
290[x1
data: 2.0
grad: ]
 --> 290mul828[mul]
828[w1
data: -0.8535535176507074
grad: ]
 --> 290mul828[mul]
290mul828[mul] --> 445[x1w1
data: -1.7071070353014148
grad: ]

 --> 445sum72[sum]
524[x2
data: 0.0
grad: ]
 --> 524mul608[mul]
608[w2
data: -0.3809673052307294
grad: ]
 --> 524mul608[mul]
524mul608[mul] --> 72[x2w2
data: -0.0
grad: ]

 --> 445sum72[sum]
445sum72[sum] --> 183[x.w
data: -1.7071070353014148
grad: ]

 --> 183add391[add]
391[b
data: -0.6926846144457646
grad: ]
 --> 183add391[add]
183add391[add] --> 676[n
data: -2.399791649747179
grad: ]

 --> 676tanh[tanh]
676tanh[tanh] --> 815[output
data: -0.9836681091430985
grad: ]



```

```elixir
Mermaid.render_value(n2)
```

<!-- livebook:{"output":true} -->

```mermaid
flowchart LR
290[x1
data: 2.0
grad: ]
 --> 290mul469[mul]
469[w1
data: 0.4316506130348108
grad: ]
 --> 290mul469[mul]
290mul469[mul] --> 863[x1w1
data: 0.8633012260696216
grad: ]

 --> 863sum437[sum]
524[x2
data: 0.0
grad: ]
 --> 524mul650[mul]
650[w2
data: 0.9677210340324143
grad: ]
 --> 524mul650[mul]
524mul650[mul] --> 437[x2w2
data: 0.0
grad: ]

 --> 863sum437[sum]
863sum437[sum] --> 535[x.w
data: 0.8633012260696216
grad: ]

 --> 535add47[add]
47[b
data: -0.2977914042283134
grad: ]
 --> 535add47[add]
535add47[add] --> 533[n
data: 0.5655098218413082
grad: ]

 --> 533tanh[tanh]
533tanh[tanh] --> 966[output
data: 0.5120540432675122
grad: ]



```

```elixir
defmodule MLP do
  defstruct [:layers]

  def new(number_of_inputs, number_of_outputs) do
    size = [number_of_inputs | number_of_outputs]
    len = Enum.count(number_of_outputs) - 1

    layers =
      for i <- 0..len do
        Layer.new(Enum.at(size, i), Enum.at(size, i + 1))
      end
    
    %__MODULE__{layers: layers}
  end

  def call(%__MODULE__{layers: layers}, inputs) do
    for layer <- layers, reduce: inputs do
      inputs ->
        Layer.call(layer, inputs)
    end
    |> List.first()
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, MLP, <<70, 79, 82, 49, 0, 0, 15, ...>>, {:call, 2}}
```

```elixir
MLP.new(2, [3, 3, 1])
|> MLP.call([x1, x2])
|> Mermaid.render_value()
```
